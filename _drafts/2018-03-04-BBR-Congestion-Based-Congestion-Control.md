---
layout: post
title: BBR：基于拥塞的拥塞控制算法（译）
tags:
    - 基础知识
---

---

BBR: Congestion-Based Congestion Control

Measuring bottleneck bandwidth and round-trip propagation time.

---

最近一周看了《TCP/IP 详解：卷一》的 TCP 部分，这本书写了有二十多年了，互联网已经发生了很大的变化，不跟进下当前最新的进展显然是不够的。而当前 TCP 公开的最新进展应该就是 BBR 了，经过再三思索、再三搜索，我决定把当初 Google 发表在 ACM 上的论文翻译为中文，当然最主要的还是为了加深自己的学习，但如果能给其他人带来一点帮助，那我也是很高兴的。

[BBR: Congestion-Based Congestion Control 原文链接](https://queue.acm.org/detail.cfm?id=3022184)。

---

BBR：基于拥塞的拥塞控制算法。测量网络瓶颈处的带宽和往返传播时间。

不管怎么说，当前的互联网传输数据的效率没有达到它应有的效率。世界上大部分蜂窝网络的用户都忍受着几秒甚至几分钟的延迟；机场或者大型会议现场的公共 WiFi 通常延迟更大。物理学家和气象学家需要和他们的全球合作伙伴交换 PB 级的数据，但却发现他们精心打造的拥有数 Gbps 带宽的网络设施，在跨洲传输时传输速度往往只能到达几 Mbps。

这些问题来自于二十世纪八十年代 TCP 拥塞控制算法设计之初的一些设计选择：把丢包等同于“拥塞”。这一等式在当时确实成立，但却是因为技术限制，而非[第一原理（first priciple）](https://zh.wikipedia.org/wiki/%E7%AC%AC%E4%B8%80%E5%8E%9F%E7%90%86)。随着 NIC（network interface controller）们的带宽从 Mbps 发展到 Gbps，内存芯片从 KB 发展到 GB，丢包和拥塞之间的关系变得更微弱了。

当前 TCP 基于丢包的拥塞控制算法——即便是当前最优秀的 [CUBIC](https://tools.ietf.org/html/rfc8312)——是这些问题的主要原因。当网络瓶颈的缓冲区大时，基于丢包的拥塞控制算法会让这些缓冲区被填满，导致缓冲区膨胀（bufferbloat）。当网络瓶颈的缓冲区小时，基于丢包的拥塞控制算法误把丢包当做拥塞的信号，导致低吞吐量。要解决这些问题，需要一种替代基于丢包的拥塞控制算法。而为了提出这样的算法，需要我们从根源上理解网络拥塞发生在哪里、如何发生。

## 拥塞与瓶颈

在任何时候，一个全双工的 TCP 连接在每个方向上都刚好有一个最慢的通道，我们称之为瓶颈（bottleneck）。瓶颈的重要性在于：

+ 它决定了连接的最大数据传输速率。这是所有通道的固有特性（例如，我们画一条六车道马路，在高峰期一场事故使其中一小段缩减为了单车道，事故上游车辆的最快通过速度就是事故处的通过速度）。
+ 它是缓冲队列开始堆积的地方。只有当链路（_节点_）的发送速率超过接收速率时，它的缓冲队列才会缩减。在一个满速传输的连接中，瓶颈的所有上游发送速率都比瓶颈的大，这就让上游的队列都转移到了瓶颈这里。

不管一个连接经过了多少条链路，也不管每条链路的速度是多少，在 TCP 看来，只要有同样的 RTT（round-trip time，往返传输时间）和瓶颈传输速率，一个复杂的路径和单个链路没有区别。它们是两个物理上的限制，RTprop（round-trip propagation time，往返传输时间）和 BtlBw（bottleneck bandwidth，瓶颈带宽）。（如果把整个网络通道当做是一个现实世界的管道，那 RTprop 就是它的长度，BtlBw 就是它的最小直径。）

图一显示了 RTT 和传输速率随 inflight 数据量（已发送但未被确认）的变化图。蓝线表示了 RTprop 的限制，绿线表示了 BtlBw 的限制，红线是瓶颈的缓冲区。阴影区域里的操作是不允许的，因为它至少违背了一个限制条件。限制条件的转换导致了三个拥有不同传输质量的区域（APP 限制，带宽限制，缓冲区限制）。

![](https://imgs.piasy.com/2018-03-04-vanjacobson1.png)

当没有足够多的 inflight 数据，不够充满管道时，RTprop 决定行为特征；否则 BtlBw 起决定作用。（_上下每幅图中_）限制条件线相交于 inflight = BtlBw × RTprop 处，亦即网络的 BDP（bandwidth-delay product）。由于在这一点之后网络就已经被充满了，超过 BDP 的 inflight 数据（_超量数据_）就在瓶颈处产生了缓冲队列，这就导致了上半幅图里的 RTT 随 inflight 数据量线性增长。如果超量数据超过了缓冲区的大小，那就会发生丢包了。**拥塞**实际上就是在 BDP 线的右侧进行运转，而**拥塞控制**就是控制一个连接在运转过程中最右位置的策略。

基于丢包的拥塞控制算法在带宽限制区域的右边缘运转，以瓶颈带宽传输数据，但代价是高延迟和频繁丢包。以前内存很昂贵，缓冲区的大小只比 BDP 大一点点，这样实际上基于丢包的拥塞控制算法带来的延迟还比较小。后来内存变得便宜了，缓冲区的大小比 BDP 大了几个数量级，由此导致的缓冲区膨胀带来了秒级的延迟，而不再是毫秒级的延迟。

带宽限制区域的左侧相比于右侧，是更优的运转点。在 1979 年 Leonard Kleinrock 说明了左侧是最优运转点，它能最大化传输带宽而保持最低的延迟，无论是对单个连接还是对全网，都是最优选择。然而也是这个时候，Jeffrey M. Jaffe 证明了我们无法设计出一个能收敛到这个最优点的分布式算法。这一结果改变了学术界的研究方向，研究者们不再寻找能达到 Kleinrock 最优点的分布式算法，而是寻找其他方案来进行拥塞控制。

我们这个 Google 的团队每天花好几个小时分析收集自世界各地的 TCP 包头，寻找异常行为。通常我们首先找出路径的关键特征、RTprop 和 BtlBw。从这些流量记录里我们可以推论出，Jaffe 的结论的限制性可能不像当初那样强烈了。他的结论依赖于根本上的测量歧义性（例如，测量出的 RTT 增加，可能是由路径长度变化导致的，也可能由瓶颈带宽降低导致，也可能是其他连接导致的排队时间增加导致）。尽管我们不可能消除单次测量的歧义性，但一个连接的长时间行为可以告诉我们更准确的信息，我们有可能设计出一个能够解决歧义问题的测量策略。

把这些测量和一个鲁棒的服务器结合起来，我们可以得到一个能响应实际拥塞的分布式拥塞控制算法，它不会被丢包或者临时排队延迟所干扰，并且会大概率收敛到 Kleinrock 最优点。于是我们花了三年时间来设计出一套拥塞控制算法，它通过测量两个指标来抽取路径的特征：瓶颈带宽和往返传输时间，即 BBR（**B**ottleneck **B**andwidth and **R**ound-trip propagation time）。

## 抽取瓶颈的特征


